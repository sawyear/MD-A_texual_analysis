# Measuring Corporate Culture Using Machine Learning â€” Chinese Adaptation ä¸­æ–‡ç‰ˆ

## ðŸ“˜ Introduction

This repository implements a **Chinese-adapted version** of the method proposed in the paper:

> Kai Li, Feng Mai, Rui Shen, Xinyan Yan,  
> [__Measuring Corporate Culture Using Machine Learning__](https://academic.oup.com/rfs/advance-article-abstract/doi/10.1093/rfs/hhaa079/5869446?redirectedFrom=fulltext),  
> _The Review of Financial Studies_, 2020.  
> DOI: [10.1093/rfs/hhaa079](http://dx.doi.org/10.1093/rfs/hhaa079)

The original implementation is designed for **English-language corporate texts** (e.g., earnings calls).  
This repository extends the pipeline to support **Chinese textual data**, including Chinese annual reports, CSR reports, and management discussions.

æœ¬é¡¹ç›®åŸºäºŽä¸Šè¿°è®ºæ–‡æ–¹æ³•è¿›è¡Œäº†ä¸­æ–‡åŒ–æ”¹é€ ï¼Œå®žçŽ°äº†ä»Žä¸­æ–‡è¯­æ–™çš„é¢„å¤„ç†ã€åˆ†è¯ã€çŸ­è¯­å­¦ä¹ ã€åˆ°è¯å‘é‡å»ºæ¨¡ä¸Žå¾—åˆ†è®¡ç®—çš„å®Œæ•´æµç¨‹ã€‚

The modified pipeline has been tested on **Windows 10**, **Ubuntu 20.04**, and **macOS Sonoma**,  
using Python 3.8â€“3.12 environments.

---

## ðŸ§© Key Modifications for Chinese Version

| Component | Original (English) | Modified (Chinese) |
|------------|--------------------|--------------------|
| Tokenization | Stanford CoreNLP (English POS/NER) | Stanford CoreNLP (Chinese POS/NER), optional **pkuseg** / **THULAC** |
| Stopwords | NLTK stopword list | Chinese stopword dictionary (`data/resources/StopWords_Generic.txt`) |
| Named Entity Removal | CoreNLP NER | Optional filtering using **HanLP** or **pkusegNER** |
| Phrase Detection | gensim Phrases (2â€“3 gram) | Retained; applied on tokenized Chinese words |
| Word Embedding | gensim Word2Vec | Compatible; trained on Chinese bigram/trigram corpus |
| Dictionary Expansion | cosine similarity in embedding space | Fully retained |
| Scoring | TF / TF-IDF / WF-IDF | Fully retained, adapted for Chinese vocabulary |
| Optional Aggregation | by firmâ€“year | Fully retained |

---

## âš™ï¸ Requirements

- Python 3.8+
- Install required packages:
  ```bash
  pip install -r requirements.txt
 ```

### Stanford CoreNLP

If you still wish to use **Stanford CoreNLP** (e.g., for dependency parsing or POS tagging in Englishâ€“Chinese mixed texts),
download [Stanford CoreNLP v4.5.4+](https://nlp.stanford.edu/software/stanford-corenlp-4.5.10-models-chinese.jar)
and specify its path in `global_options.py`:

```python
os.environ["CORENLP_HOME"] = "/your/path/stanford-corenlp-4.5.10/"
```
---

## ðŸ“ Data Format

Example input files should be placed in `data/input/`:

| File                        | Description                                                                                                     |
| --------------------------- | --------------------------------------------------------------------------------------------------------------- |
| `documents.txt`             | Each line is a full document (e.g., one annual report, CSR report, or MD&A text).  |
| `document_ids.txt`          | Each line is a unique document ID (e.g., firmâ€“year).                              |
| `id2firms.csv` *(optional)* | CSV with columns: `document_id` (str), `firm_id` (str), `time` (int). Used for firmâ€“time aggregation.           |

---

## âš™ï¸ Global Options

Edit `global_options.py` to set:

* RAM and CPU cores for training and parsing
* Stopword file path (`data/resources/StopWords_Generic.txt`)
* Seed word list (see `data/dictionaries/seed_words.csv`)
* Max number of words per cultural dimension
* Whether to enable bigram/trigram detection

---

## ðŸš€ Running the Code

### 1ï¸âƒ£ Text Parsing / Tokenization

Run:

```bash
python parse.py
```

This step performs **Chinese word segmentation**, **optional NER removal**, and saves segmented results to:

```
data/processed/parsed/
```

Output files:

* `documents_tokenized.txt`: segmented sentences (one per line)
* `document_sent_ids.txt`: documentâ€“sentence mapping IDs

---

### 2ï¸âƒ£ Cleaning and Training Word2Vec

Run:

```bash
python clean_and_train.py
```

This script:

* Removes stopwords and numerics
* Learns bigrams and trigrams using `gensim.models.Phrases`
* Trains a Chinese Word2Vec model

Outputs:

* `models/w2/w2v.mod`
* `models/phrases/bigram.mod`
* `models/phrases/trigram.mod`

---

### 3ï¸âƒ£ Dictionary Expansion

Run:

```bash
python create_dict.py
```

It expands the seed dictionary (e.g., integrity, teamwork, innovation, respect, and customer orientation)
using cosine similarity in the embedding space.

Output:

```
outputs/dict/expanded_dict.csv
```

---

### 4ï¸âƒ£ Scoring Documents

Run:

```bash
python score.py
```

Generates culture scores for each document using three schemes:

* **TF** (term frequency)
* **TF-IDF**
* **WF-IDF** (log-scaled TF-IDF)

Outputs:

```
outputs/scores/scores_TF.csv
outputs/scores/scores_TFIDF.csv
outputs/scores/scores_WFIDF.csv
```

---

### 5ï¸âƒ£ (Optional) Aggregation

Aggregate to firmâ€“year level:

```bash
python aggregate_firms.py
```

Output:

```
outputs/scores/firm_scores_TF.csv
outputs/scores/firm_scores_TFIDF.csv
outputs/scores/firm_scores_WFIDF.csv
```

---

## ðŸ“š Notes and Tips

* Recommended to clean Chinese input files with UTF-8 encoding (remove HTML tags, control characters).
* For large corpora, training can be parallelized by increasing `workers` in `global_options.py`.
* You may extend the seed dictionary to domain-specific dimensions (e.g., ESG, innovation, compliance).

---

## ðŸ§  Citation

If you use this repository, please cite the original paper:

> Li, Kai, Feng Mai, Rui Shen, and Xinyan Yan.
> *Measuring Corporate Culture Using Machine Learning.*
> *The Review of Financial Studies*, 2020.
> DOI: [10.1093/rfs/hhaa079](https://doi.org/10.1093/rfs/hhaa079)

And please mention the Chinese adaptation (this repository).

## ðŸ§  Example Workflow

```bash
python parse.py
python clean_and_train.py
python create_dict.py
python score.py
python aggregate_firms.py
```

After running, check:

* `outputs/scores/scores_TF.csv` â†’ document-level scores
* `outputs/scores/scores_TFIDF.csv` â†’ document-level scores
* `outputs/scores/scores_WFIDF.csv` â†’ document-level scores
* `outputs/scores/firm_scores_TF.csv` â†’ firmâ€“year aggregated indices
* `outputs/scores/firm_scores_TFIDF.csv` â†’ firmâ€“year aggregated indices
* `outputs/scores/firm_scores_WFIDF.csv` â†’ firmâ€“year aggregated indices

---

## ðŸ§© Acknowledgement

This Chinese adaptation builds upon the original RFS 2020 paper and open-source code.
All modifications are intended to enable cross-lingual and domain adaptation for Chinese corporate texts.

---

### ðŸ”– License

This project is distributed under the same license as the original repository (see `LICENSE` file).
Please attribute the original authors and this adaptation appropriately.

```

